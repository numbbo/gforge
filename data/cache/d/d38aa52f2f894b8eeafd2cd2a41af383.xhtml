
<h1 class="sectionedit1"><a name="topics_for_the_discussion_at_bbob_2012" id="topics_for_the_discussion_at_bbob_2012">Topics for the discussion at BBOB 2012</a></h1>
<div class="level1">
<ol>
<li class="level1"><div class="li"> Where is BBOB/COCO going?</div>
<ul>
<li class="level2"><div class="li"> What is BBOB 2013/14 going to be about? </div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> What extensions are planned? What extensions do you wish? What should their priorities be?</div>
<ul>
<li class="level2"><div class="li"> Short runs / limited budget?</div>
</li>
<li class="level2"><div class="li"> Constrained optimization?</div>
</li>
<li class="level2"><div class="li"> Large scale optimization?</div>
</li>
<li class="level2"><div class="li"> Multi-objective optimization?</div>
</li>
<li class="level2"><div class="li"> Real-world problems?</div>
<ul>
<li class="level3"><div class="li"> Which ones? </div>
</li>
<li class="level3"><div class="li"> They are usually not scalable…</div>
</li>
<li class="level3"><div class="li"> We often do not know their optimal solution…</div>
</li>
</ul>
</li>
<li class="level2"><div class="li"> Are the functions representative of real world problems? Do we currently miss any important traits?</div>
</li>
<li class="level2"><div class="li"> Landscape analysis </div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> What do you dislike (or even hate) about BBOB/COCO?</div>
</li>
<li class="level1"><div class="li"> During the optimization, algorithms sample points in the search space. Based on them they should provide the user with the <strong>recommendation</strong>, an estimate of the optimal solution. This recommendation part is currently ignored by the COCO evaluation process.</div>
<ul>
<li class="level2"><div class="li"> Are recommendations necessary to make an algorithm performance evaluation (keyword explicit recommendations)?</div>
<ul>
<li class="level3"><div class="li"> In the noiseless case?</div>
</li>
<li class="level3"><div class="li"> In the noisy case? </div>
</li>
</ul>
</li>
<li class="level2"><div class="li"> How are the algorithms without recommendations evaluated in a framework where recommendations are used for evaluation (keyword implicit recommendations)?</div>
</li>
<li class="level2"><div class="li"> Do we see explicit recommendations as a complement or as a replacement of the samples produced by an algorithm?</div>
</li>
<li class="level2"><div class="li"> How is the performance computed from a sequence of solutions (each possibly associated with a noisy f-evaluation)?</div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> Is the COCO methodology limited to real-valued domain?</div>
<ul>
<li class="level2"><div class="li"> What are the required conditions for the app domain to use the COCO methodology?</div>
</li>
<li class="level2"><div class="li"> Is the COCO benchmarking methodology actually suitable for other domains?</div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> Statistical comparisons of algorithms:</div>
<ul>
<li class="level2"><div class="li"> COCO can compare 2 algos against each other or several algos against the best of BBOB 2009</div>
</li>
<li class="level2"><div class="li"> Can it be done better? Is something missing?</div>
</li>
<li class="level2"><div class="li"> Feature request: User choice of the baseline algorithm when comparing several algos?</div>
</li>
<li class="level2"><div class="li"> Feature request: Comparison of all pairs of alos in the study?</div>
</li>
<li class="level2"><div class="li"> Feature request: A support for a systematic analysis of the algorithm parameters sensitivity?</div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> Suggestions for implementation changes/improvements:</div>
<ul>
<li class="level2"><div class="li"> Improved modularization with clearly defined interfaces between the modules</div>
</li>
<li class="level2"><div class="li"> Make more settings part of the command-line scripts arguments:</div>
<ul>
<li class="level3"><div class="li"> The choice of line patterns for the graphs</div>
</li>
<li class="level3"><div class="li"> The choice of the baseline algorithm when comparing more than two algorithms</div>
</li>
</ul>
</li>
</ul>
</li>
</ol>

</div>
<!-- EDIT1 SECTION "Topics for the discussion at BBOB 2012" [1-2551] -->
<h2 class="sectionedit2"><a name="feature_requests" id="feature_requests">Feature requests</a></h2>
<div class="level2">

</div>
<!-- EDIT2 SECTION "Feature requests" [2552-2581] -->
<h3 class="sectionedit3"><a name="the_ability_to_use_line_patterns_specified_by_the_user" id="the_ability_to_use_line_patterns_specified_by_the_user">The ability to use line patterns specified by the user</a></h3>
<div class="level3">

<p>
For the comparison of more than 2 algorithms, I would appreciate the ability to define my own line patterns for the graphs. Why? If I compare several unrelated algos, it is OK to have them each plotted with a different color… But, if I have e.g. 6 algorithms which are actually 6 instances of the same algorithm differing only in the levels of e.g. 2 factors, say popsize (large, medium, small) and crossover (on, off), than it would be highly desirable to encode the popsize e.g. by 3 different markers, and the presence of crossover e.g. by the line type (solid, dashed).
</p>

<p>
Current solution (not sure to which graphs this applies): the variable line_styles in file genericsettings.py defines the line styles. The simplest solution is therefore to assign this variable with a different value. An example will be provided in the file in the next release. 
</p>

</div>
<!-- EDIT3 SECTION "The ability to use line patterns specified by the user" [2582-3504] -->
<h2 class="sectionedit4"><a name="tutorial_requests" id="tutorial_requests">Tutorial requests</a></h2>
<div class="level2">

</div>
<!-- EDIT4 SECTION "Tutorial requests" [3505-3535] -->
<h3 class="sectionedit5"><a name="how_to_build_my_own_algorithm_performance_index_using_the_coco_api" id="how_to_build_my_own_algorithm_performance_index_using_the_coco_api">How to build my own &quot;algorithm performance index&quot; using the COCO API?</a></h3>
<div class="level3">

<p>

Provide the user with means (and tutorial) how to easilly define 1 definitive criterion that could be used to rank the algorithms. In the docs, it is actually suggested to use COCO to explore various parameter settings of the algorithms. But what COCO provides to the user with its automatically generated graphs and tables is usually a kind of “feeling” which algorithm works best and this feeling is based on some nonarticulated criterion the user implicitly uses.
</p>

<p>
This issue will be covered by a short tutorial during BBOB if time permits. We will show how to access the experimental data using the COCO DataSet and DataSetList classes and we will show how to aggregate them using (variants of) the geometric mean as the performance index.
</p>

</div>
<!-- EDIT5 SECTION "How to build my own algorithm performance index using the COCO API?" [3536-4361] -->
<h2 class="sectionedit6"><a name="faqs" id="faqs">FAQs</a></h2>
<div class="level2">

<p>
Things worth repeating over and over.
</p>

</div>
<!-- EDIT6 SECTION "FAQs" [4362-4417] -->
<h3 class="sectionedit7"><a name="how_to_interpret_the_ecdf_lines_on_the_right_from_the_big_cross" id="how_to_interpret_the_ecdf_lines_on_the_right_from_the_big_cross">How to interpret the ECDF lines on the right from the big cross?</a></h3>
<div class="level3">

<p>

Q: If I benchmarked 2 algorithms with the same evaluations budget and they both solved the same proportion of probles (as indicated by the crosses in the ECDF graphs which are virtually in the same place), how come that the curve on the right from the cross rises significantly for one of the algorithms, while it stagnates for the other algorithm?
</p>

<p>
A: This happens if one algorithm has solved for some functions on each of them only a few instances, while another solves all instances on one of these functions, and none on the others. Before the cross is reached, their performance could look similar (they solve the same overall number of instances). However, as the ECDF graphs are derived from virtual restarts on the same function (but with random instances), the first algorithm will raise and appear to solve all functions after the cross, whereas the second will stagnate as it only solved one of the functions.  

</p>

</div>
<!-- EDIT7 SECTION "How to interpret the ECDF lines on the right from the big cross?" [4418-] -->